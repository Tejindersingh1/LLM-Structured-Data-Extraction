{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7959844,"sourceType":"datasetVersion","datasetId":4682356}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install git+https://github.com/huggingface/transformers accelerate\n!pip -q install qwen_vl_utils\n!pip install bitsandbytes --upgrade --quiet\n!pip install -q \"outlines[transformers]\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:24:34.531635Z","iopub.execute_input":"2025-04-26T16:24:34.531830Z","iopub.status.idle":"2025-04-26T16:26:32.351650Z","shell.execute_reply.started":"2025-04-26T16:24:34.531813Z","shell.execute_reply":"2025-04-26T16:26:32.350715Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from PIL import Image\nimport pandas as pd\nfrom pydantic import BaseModel\nimport json\nimport re\n\nimport base64\nimport torch\n#from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor, BitsAndBytesConfig\nfrom qwen_vl_utils import process_vision_info\nfrom outlines import models as om\nfrom outlines import generate as og\nprint(\"Libraries Imported\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:31:00.011127Z","iopub.execute_input":"2025-04-26T16:31:00.011429Z","iopub.status.idle":"2025-04-26T16:31:00.295215Z","shell.execute_reply.started":"2025-04-26T16:31:00.011406Z","shell.execute_reply":"2025-04-26T16:31:00.294415Z"}},"outputs":[{"name":"stdout","text":"Libraries Imported\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# Directory containing the PDF files\nimg_folder = '/kaggle/input/illegible-medical-prescription-images-dataset/data'\noutput_excel = '/kaggle/working/qwen2-5-7b.xlsx'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:30:05.402365Z","iopub.execute_input":"2025-04-26T16:30:05.402849Z","iopub.status.idle":"2025-04-26T16:30:05.406518Z","shell.execute_reply.started":"2025-04-26T16:30:05.402830Z","shell.execute_reply":"2025-04-26T16:30:05.405709Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # Enable 4-bit quantization\n    bnb_4bit_quant_type=\"nf4\",    # Use Normal Float 4 (NF4) quantization\n    bnb_4bit_use_double_quant=True,  # Double quantization for better efficiency\n    bnb_4bit_compute_dtype=torch.float16  # Use float16 for computation\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:30:05.407267Z","iopub.execute_input":"2025-04-26T16:30:05.407467Z","iopub.status.idle":"2025-04-26T16:30:05.429301Z","shell.execute_reply.started":"2025-04-26T16:30:05.407450Z","shell.execute_reply":"2025-04-26T16:30:05.428437Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"MODEL_ID = \"Qwen/Qwen2.5-VL-7B-Instruct\" \nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Loading multimodal model {MODEL_ID} on {DEVICE}...\")\n# ── Load & wrap the multimodal model with Outlines ────────────────────────\nprint(f\"Loading {MODEL_ID} with Outlines vision wrapper …\")\nmodel = om.transformers_vision(\n    MODEL_ID,\n    model_class=Qwen2_5_VLForConditionalGeneration,  # correct HF class\n    device=None,\n    model_kwargs={           # forwarded to .from_pretrained(...)\n        \"device_map\": \"auto\",\n        \"quantization_config\": bnb_config,\n        \"torch_dtype\": \"auto\",\n    },\n    tokenizer_class=AutoTokenizer,\n)\nprint(\"✅ Model ready.\")\nprocessor = AutoProcessor.from_pretrained(MODEL_ID)\nprint(\"Multimodal model loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:31:04.441544Z","iopub.execute_input":"2025-04-26T16:31:04.441886Z","iopub.status.idle":"2025-04-26T16:33:01.600446Z","shell.execute_reply.started":"2025-04-26T16:31:04.441864Z","shell.execute_reply":"2025-04-26T16:33:01.599825Z"}},"outputs":[{"name":"stdout","text":"Loading multimodal model Qwen/Qwen2.5-VL-7B-Instruct on cuda...\nLoading Qwen/Qwen2.5-VL-7B-Instruct with Outlines vision wrapper …\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e27a6295f2414be69cbaf7eade449dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/57.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ba21615db1b4046b22655b3c0027c36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e62e92a7a8634a35949d7027f2a8ecdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/3.90G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b306e8469b784d4ebc16e3e5adaa17c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"547072e5156344d496ae639b8872bf91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322ce435d6bb442eb9a654b9cc948588"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be9fa866a7d64f22bcfa4134d0afe46a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af856b8a2b7447faccee6621cd1e06f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b96e984ef9894c3cbb1d9114bbc0f5a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44eae7c1ee324593b39f50a6dad053a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0e920419bb4291ae3115994f7a797c"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/5.70k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f043e031e643d2a4a579f41b1dc756"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d82c285f5544c299836ceecd6adfcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd9598ee0ef644e7ba4c58f3653c76b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ea5e68de34b4d7c923c0d8e172725e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569baaf236524ec1a20f5edf64cce918"}},"metadata":{}},{"name":"stdout","text":"✅ Model ready.\nMultimodal model loaded successfully!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#from pydantic import BaseModel\n\n\nclass ClinicalRecord(BaseModel):\n    patient_name: str\n    diagnosis: str\n    prescription: str\n\n\njson_gen = og.json(model, ClinicalRecord)\nprint(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:34:01.636155Z","iopub.execute_input":"2025-04-26T16:34:01.636702Z","iopub.status.idle":"2025-04-26T16:34:05.545974Z","shell.execute_reply.started":"2025-04-26T16:34:01.636668Z","shell.execute_reply":"2025-04-26T16:34:05.545319Z"}},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def generate_from_multimodal(prompt_text, images, max_tokens, temperature, top_p):\n    \"\"\"Generate text from multimodal inputs using Qwen2-VL\"\"\"\n    record = json_gen(\n        prompt_text,   # raw chat template with an [IMG] token\n        images         # List[PIL.Image.Image]\n    )\n\n    return record\n    \n\n\ndef create_chat_completion(messages_dict):\n    prompt = processor.apply_chat_template(\n        messages_dict, tokenize=False, add_generation_prompt=True\n    )\n    images, videos = process_vision_info(messages_dict)\n    # Generate response using the prompt and images\n    response_text = generate_from_multimodal(\n        prompt_text=prompt,\n        images=images, #processed_content.get(\"images\", []),\n        max_tokens=2048,\n        temperature= 1,\n        top_p=1\n    )\n\n    return response_text\n\nprint(\"Done\")\n\n\ndef encode_image(image):\n    buffered = io.BytesIO()\n    image.save(buffered, format=\"PNG\")\n    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return img_str\n\ndef check_and_resize_image(image, max_width=256, max_height=512):\n    width, height = image.size\n    # Check if the image resolution exceeds the max allowed\n    if width > max_width or height > max_height:\n        # Calculate the new dimensions maintaining the aspect ratio\n        aspect_ratio = width / height\n        if width > height:\n            new_width = max_width\n            new_height = int(new_width / aspect_ratio)\n        else:\n            new_height = max_height\n            new_width = int(new_height * aspect_ratio)\n        \n        # Resize the image\n        image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n    \n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:44:59.661301Z","iopub.execute_input":"2025-04-26T16:44:59.661599Z","iopub.status.idle":"2025-04-26T16:44:59.668797Z","shell.execute_reply.started":"2025-04-26T16:44:59.661576Z","shell.execute_reply":"2025-04-26T16:44:59.668016Z"}},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"variable_list = [\"patient_name\", \"diagnosis\", \"prescription\"] \n\n\n# Function to send multiple images in a single API call\ndef extract_structured_data_from_images(images):\n    # Encode all images\n    images_ = [check_and_resize_image(image) for image in images]\n    base64_images = [encode_image(image) for image in images_]\n    # Create message content with all the images embedded\n    message_content = [\n        \n        {\n            \"type\": \"text\",\n            \"text\": f\"Act as a Doctor and read images carefully. Extract the following clinical variables from the provided images and structure them according to these categories: {variable_list}. If any variable is not present in the image, return 'Not specified'.\" \n        }\n    ]\n\n    # Add each image to the message content\n    for base64_image in base64_images:\n        message_content.append({\n            \"type\": \"image\",\n            \"image\": f\"data:image/jpeg;base64,{base64_image}\"\n        })\n    \n    message = [\n            {\n                \"role\": \"system\",\n                \"content\": [\n                    {\"type\": \"text\",\n                     \"text\" : \"\"\"Extract the text elements described by the user from the pictures, and return the result in structured JSON format : {variable_name : value}\n                             Provide output for each variable strictly as it is mentioned. Strictly ensure that the variable names in output must be identicle to input variable list.     \n                            Return only valid JSON with no extra explanations.\n                            \"\"\"\n                    }\n                ]\n            },\n            {\n                \"role\": \"user\",\n                \"content\": message_content\n            },\n        \n        ]\n\n    # Call the GPT-4o-mini model via OpenAI API\n    structured_info = create_chat_completion(message)\n    \n    return structured_info\n\nprint(\"Done\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:45:15.787400Z","iopub.execute_input":"2025-04-26T16:45:15.787701Z","iopub.status.idle":"2025-04-26T16:45:15.794305Z","shell.execute_reply.started":"2025-04-26T16:45:15.787678Z","shell.execute_reply":"2025-04-26T16:45:15.793438Z"}},"outputs":[{"name":"stdout","text":"Done\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\nimport time\nimport io\nimport gc\nstructured_data = []\n\n# Loop through all image files in the folder\nfor idx, img_file in enumerate(sorted(os.listdir(img_folder)), 1):\n    if not img_file.lower().endswith(('.jpg')):\n        continue\n\n    img_path = os.path.join(img_folder, img_file)\n    print(f\"[{idx}] Processing {img_file}…\")\n    start_time = time.time()\n\n    # 1) Load the image\n    image = Image.open(img_path)\n\n    # 2) Run your GPT-based extractor\n    torch.cuda.empty_cache()\n    gc.collect()\n    extracted_info = extract_structured_data_from_images([image])\n    extraction_time = time.time() - start_time\n\n    record = {\n        'Image_ID': os.path.splitext(img_file)[0],\n        'Extraction_Time_s': extraction_time,\n        **extracted_info.dict() #(parsed if isinstance(parsed, dict) else {'parsed': parsed})\n    }\n    structured_data.append(record)\n\n# 6) Save to Excel\ndf = pd.DataFrame(structured_data)\ndf.to_excel(output_excel, index=False)\nprint(f\"Done! Results saved to {output_excel}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T16:45:16.161128Z","iopub.execute_input":"2025-04-26T16:45:16.161367Z","iopub.status.idle":"2025-04-26T17:00:37.230042Z","shell.execute_reply.started":"2025-04-26T16:45:16.161349Z","shell.execute_reply":"2025-04-26T17:00:37.229418Z"}},"outputs":[{"name":"stdout","text":"[1] Processing 1.jpg…\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1127285340.py:28: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  **extracted_info.dict() #(parsed if isinstance(parsed, dict) else {'parsed': parsed})\n","output_type":"stream"},{"name":"stdout","text":"[2] Processing 10.jpg…\n[3] Processing 100.jpg…\n[4] Processing 101.jpg…\n[5] Processing 102.jpg…\n[6] Processing 103.jpg…\n[7] Processing 104.jpg…\n[8] Processing 105.jpg…\n[9] Processing 106.jpg…\n[10] Processing 107.jpg…\n[11] Processing 108.jpg…\n[12] Processing 109.jpg…\n[13] Processing 11.jpg…\n[14] Processing 110.jpg…\n[15] Processing 111.jpg…\n[16] Processing 112.jpg…\n[17] Processing 113.jpg…\n[18] Processing 114.jpg…\n[19] Processing 115.jpg…\n[20] Processing 116.jpg…\n[21] Processing 117.jpg…\n[22] Processing 118.jpg…\n[23] Processing 119.jpg…\n[24] Processing 12.jpg…\n[25] Processing 120.jpg…\n[26] Processing 121.jpg…\n[27] Processing 122.jpg…\n[28] Processing 123.jpg…\n[29] Processing 124.jpg…\n[30] Processing 125.jpg…\n[31] Processing 126.jpg…\n[32] Processing 127.jpg…\n[33] Processing 128.jpg…\n[34] Processing 129.jpg…\n[35] Processing 13.jpg…\n[36] Processing 14.jpg…\n[37] Processing 15.jpg…\n[38] Processing 16.jpg…\n[39] Processing 17.jpg…\n[40] Processing 18.jpg…\n[41] Processing 19.jpg…\n[42] Processing 2.jpg…\n[43] Processing 20.jpg…\n[44] Processing 21.jpg…\n[45] Processing 22.jpg…\n[46] Processing 23.jpg…\n[47] Processing 24.jpg…\n[48] Processing 25.jpg…\n[49] Processing 26.jpg…\n[50] Processing 27.jpg…\n[51] Processing 28.jpg…\n[52] Processing 29.jpg…\n[53] Processing 3.jpg…\n[54] Processing 30.jpg…\n[55] Processing 31.jpg…\n[56] Processing 32.jpg…\n[57] Processing 33.jpg…\n[58] Processing 34.jpg…\n[59] Processing 35.jpg…\n[60] Processing 36.jpg…\n[61] Processing 37.jpg…\n[62] Processing 38.jpg…\n[63] Processing 39.jpg…\n[64] Processing 4.jpg…\n[65] Processing 40.jpg…\n[66] Processing 41.jpg…\n[67] Processing 42.jpg…\n[68] Processing 43.jpg…\n[69] Processing 44.jpg…\n[70] Processing 45.jpg…\n[71] Processing 46.jpg…\n[72] Processing 47.jpg…\n[73] Processing 48.jpg…\n[74] Processing 49.jpg…\n[75] Processing 5.jpg…\n[76] Processing 50.jpg…\n[77] Processing 51.jpg…\n[78] Processing 52.jpg…\n[79] Processing 53.jpg…\n[80] Processing 54.jpg…\n[81] Processing 55.jpg…\n[82] Processing 56.jpg…\n[83] Processing 57.jpg…\n[84] Processing 58.jpg…\n[85] Processing 59.jpg…\n[86] Processing 6.jpg…\n[87] Processing 60.jpg…\n[88] Processing 61.jpg…\n[89] Processing 62.jpg…\n[90] Processing 63.jpg…\n[91] Processing 64.jpg…\n[92] Processing 65.jpg…\n[93] Processing 66.jpg…\n[94] Processing 67.jpg…\n[95] Processing 68.jpg…\n[96] Processing 69.jpg…\n[97] Processing 7.jpg…\n[98] Processing 70.jpg…\n[99] Processing 71.jpg…\n[100] Processing 72.jpg…\n[101] Processing 73.jpg…\n[102] Processing 74.jpg…\n[103] Processing 75.jpg…\n[104] Processing 76.jpg…\n[105] Processing 77.jpg…\n[106] Processing 78.jpg…\n[107] Processing 79.jpg…\n[108] Processing 8.jpg…\n[109] Processing 80.jpg…\n[110] Processing 81.jpg…\n[111] Processing 82.jpg…\n[112] Processing 83.jpg…\n[113] Processing 84.jpg…\n[114] Processing 85.jpg…\n[115] Processing 86.jpg…\n[116] Processing 87.jpg…\n[117] Processing 88.jpg…\n[118] Processing 89.jpg…\n[119] Processing 9.jpg…\n[120] Processing 90.jpg…\n[121] Processing 91.jpg…\n[122] Processing 92.jpg…\n[123] Processing 93.jpg…\n[124] Processing 94.jpg…\n[125] Processing 95.jpg…\n[126] Processing 96.jpg…\n[127] Processing 97.jpg…\n[128] Processing 98.jpg…\n[129] Processing 99.jpg…\nDone! Results saved to /kaggle/working/qwen2-5-7b.xlsx\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-26T17:08:20.353065Z","iopub.execute_input":"2025-04-26T17:08:20.353814Z","iopub.status.idle":"2025-04-26T17:08:20.403489Z","shell.execute_reply.started":"2025-04-26T17:08:20.353790Z","shell.execute_reply":"2025-04-26T17:08:20.402848Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"    Image_ID  Extraction_Time_s                    patient_name  \\\n0          1           5.488448                      Patient 30   \n1         10           5.003311                   Mr. CR SAMUEL   \n2        100           7.174057                 Dr. W. F. Baker   \n3        101           2.943836                       To change   \n4        102           5.384174                   Not specified   \n..       ...                ...                             ...   \n124       95           7.144327  BABY/SWLI DUBEY 01-2646-132005   \n125       96           4.344285                   Kevin Zionack   \n126       97           9.441022                      John Smith   \n127       98           7.706424                   Sh立场ndu Kumar   \n128       99           6.136859                 M. Akila Vanvar   \n\n                            diagnosis  \\\n0                       Not specified   \n1         shl vere sepiosis with MODS   \n2                       Not specified   \n3                       Not specified   \n4        No (U)String seen, 1UU Holes   \n..                                ...   \n124  Bilateral Profound Hearing Loss.   \n125                     Not specified   \n126                     Not specified   \n127                     Not specified   \n128      Facial pain with mold growth   \n\n                                          prescription  \n0                     Diprofen 0.125 mg tablet da no.7  \n1                                        Not specified  \n2    F.o. drambol. Lb. gr. 1/6. gr. 9/15-3 sty. cro...  \n3                                        Not specified  \n4    Ultraound to check positioning UC, RT after US...  \n..                                                 ...  \n124  CT scan temporal bone & MRI - Cochlear Implant...  \n125  Ferrrous Sulfate 4 mg PO TID; Dispense one bottle  \n126  Betaloc 100mg - 1 tab BID, Dorzolamidum 10 mg ...  \n127  Inj. RENDESIN 200mg in Day 1 Followed by Do an...  \n128         Liposomal Amphotericin B 300 mg - 190 tabs  \n\n[129 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Image_ID</th>\n      <th>Extraction_Time_s</th>\n      <th>patient_name</th>\n      <th>diagnosis</th>\n      <th>prescription</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>5.488448</td>\n      <td>Patient 30</td>\n      <td>Not specified</td>\n      <td>Diprofen 0.125 mg tablet da no.7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10</td>\n      <td>5.003311</td>\n      <td>Mr. CR SAMUEL</td>\n      <td>shl vere sepiosis with MODS</td>\n      <td>Not specified</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>100</td>\n      <td>7.174057</td>\n      <td>Dr. W. F. Baker</td>\n      <td>Not specified</td>\n      <td>F.o. drambol. Lb. gr. 1/6. gr. 9/15-3 sty. cro...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>101</td>\n      <td>2.943836</td>\n      <td>To change</td>\n      <td>Not specified</td>\n      <td>Not specified</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>102</td>\n      <td>5.384174</td>\n      <td>Not specified</td>\n      <td>No (U)String seen, 1UU Holes</td>\n      <td>Ultraound to check positioning UC, RT after US...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>95</td>\n      <td>7.144327</td>\n      <td>BABY/SWLI DUBEY 01-2646-132005</td>\n      <td>Bilateral Profound Hearing Loss.</td>\n      <td>CT scan temporal bone &amp; MRI - Cochlear Implant...</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>96</td>\n      <td>4.344285</td>\n      <td>Kevin Zionack</td>\n      <td>Not specified</td>\n      <td>Ferrrous Sulfate 4 mg PO TID; Dispense one bottle</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>97</td>\n      <td>9.441022</td>\n      <td>John Smith</td>\n      <td>Not specified</td>\n      <td>Betaloc 100mg - 1 tab BID, Dorzolamidum 10 mg ...</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>98</td>\n      <td>7.706424</td>\n      <td>Sh立场ndu Kumar</td>\n      <td>Not specified</td>\n      <td>Inj. RENDESIN 200mg in Day 1 Followed by Do an...</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>99</td>\n      <td>6.136859</td>\n      <td>M. Akila Vanvar</td>\n      <td>Facial pain with mold growth</td>\n      <td>Liposomal Amphotericin B 300 mg - 190 tabs</td>\n    </tr>\n  </tbody>\n</table>\n<p>129 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":23}]}